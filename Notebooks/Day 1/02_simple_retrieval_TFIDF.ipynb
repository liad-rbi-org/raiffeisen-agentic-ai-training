{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval with TF-IDF and BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31f556f-018c-4997-9116-235c7f7530b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Iterator, Tuple\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3cc1975-fa8c-47c5-a586-55481d838f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating a Toy-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58eb93cb-93a1-4563-aed8-7dc0f8aba1ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Tech Stocks Rally as Investors Bet on Strong Quarterly Earnings\",\n",
    "    \"Global Markets Dip Amid Concerns Over Slowing Tech Sector Growth\",\n",
    "    \"Cryptocurrency Prices Surge Following Regulatory Clarity in Europe\",\n",
    "    \"Federal Reserve Hints at Future Rate Cuts Boosting Market Confidence\",\n",
    "    \"Oil Prices Fall as Supply Chain Disruptions Ease Worldwide\",\n",
    "    \"Major Bank Reports Record Profits Driven by Consumer Lending\",\n",
    "    \"Retail Stocks Drop After Weak Holiday Sales Forecast\",\n",
    "    \"Automakers Invest Heavily in Electric Vehicles to Stay Competitive\",\n",
    "    \"Investors Pull Back from Risky Assets Amid Inflation Fears\",\n",
    "    \"Fintech Startups Gain Momentum with New Digital Payment Solutions\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e13bc61a-ccf5-4f3d-aa6d-c04e76fefa74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stop Word Removal and Stemming\n",
    "\n",
    "In real-world scenario, we must compromize between the model accuracy and its performance in terms of speed and space. One of the main factors to consuming space is the vocabulary size, the number of different tokens (types) we take into account.\n",
    "\n",
    "There are two main techniques to reduce the number of types:\n",
    "1. Removing stop words\n",
    "2. Stemming and Lemmatization\n",
    "\n",
    "Let's check them out:\n",
    "\n",
    "### Stop Word Removal\n",
    "Stop words are common words in a language (like ```\"the\"```, ```\"is\"```, ```\"and\"```) that appear frequently but carry little semantic meaning. Removing them reduces noise, decrease the number of tokens we must store, and therefore may improve the retrieval efficiency.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Original: ```\"The cat is sitting on the mat.\"```  \n",
    "After Stop Word Removal: ```\"cat sitting mat\"```\n",
    "\n",
    "### Stemming & Lemmatization\n",
    "[*Stemming*](https://www.geeksforgeeks.org/nlp/snowball-stemmer-nlp/) reduces words to their root form by chopping them abruptly, so that different variants of a word are treated as the same token. This helps in reducing the dimensionality, the number of different words we treat, as well as matching similar concepts and words, as they are all mapped to the same word.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Words: ```\"Consult\", \"Consultant\", \"Consulting\", \"Consultantative\", \"Consultants\"```  \n",
    "After Stemming: ```\"consult\"```\n",
    "\n",
    "*Lemmatization* reduces words to their base or dictionary form (aka lemma). Unlike stemming, which simply chops off the end of a word, lemmatization ensure the resulting lemma is a valid word. For example, \"better\" is lemmatized to \"good\" instead of a non-dictionary root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "715aa317-d14f-4ba9-9e0c-7df46c585935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(TfidfVectorizer(stop_words='english').get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1:\n",
    "\n",
    "Complete the preprocessing function in the code chunk below.\n",
    "\n",
    "The input of the function is a string of text.\n",
    "It returns a list of stemmed, non-stop words.\n",
    "\n",
    "1. Split the text into single words. (Hint: use the [```split()```](https://docs.python.org/3/library/stdtypes.html#str.split) function)\n",
    "2. For each word word check if it is a stop word using the ```stop_words``` set.\n",
    "3. Finally stem all the remaining words using [```stemmer.stem()```](https://www.nltk.org/api/nltk.stem.porter.html#nltk.stem.porter.PorterStemmer.stem).\n",
    "4. Return the resulting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Preprocess the input text by tokenizing, removing stopwords, and stemming.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: A list of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    tokens = text.lower().split()\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess documents\n",
    "processed_docs = [' '.join(preprocess(doc)) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "633470cc-81e2-46dc-ad80-b68774844743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## TF-IDF: Term Frequency–Inverse Document Frequency\n",
    "\n",
    "**TF-IDF** is a numerical statistic that reflects how important a word is to a document within a collection (corpus).  \n",
    "It is widely used in information retrieval and text mining to rank documents by relevance.\n",
    "\n",
    "**1. Term Frequency (TF)**\n",
    "\n",
    "Measures how often a term $t$ appears in a document $d$.\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in } d}{\\text{Total number of terms in } d}\n",
    "$$\n",
    "\n",
    "**2. Inverse Document Frequency (IDF)**\n",
    "\n",
    "Measures how informative a term is, by counting the number of documents it appears in — rare terms get a higher score.\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{num of documents containing term t} + 1}\\right)\n",
    "$$\n",
    "\n",
    "(The $+1$ prevents division by zero.)\n",
    "\n",
    "**3. TF-IDF Score**\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "* **High TF-IDF** → The term appears frequently in the document but rarely elsewhere → *important*.\n",
    "* **Low TF-IDF** → The term is common across documents or rare in the current one → *less informative*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF Vectorization\n",
    "\n",
    "Now it is time to apply the TF-IDF vectorization to our preprocessed data. Fortunately, we don't have to code this ourselfs.  \n",
    "The vectorization is implemented for example in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "To perform retrieval, we preprocess and vectorize the query with the same TF-IDF model that we fit on the documents.  \n",
    "Then, a distance metric, such as [*cosine similarity*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html), is used to measure the relevancy of each document, and find the best matching results.\n",
    "\n",
    "$$\\text{Cosine similarity}(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7eaf566-21d4-4be4-a562-d67210d924a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create and fit a TfidfVectorizer over the document corpus\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db13673d-6a94-4b51-b47d-aa99bf411aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: \"energi suppli price drop\"\n",
      "TF-IDF Ranking:\n",
      "Score: 0.3757 | Doc: Oil Prices Fall as Supply Chain Disruptions Ease Worldwide\n",
      "Score: 0.2337 | Doc: Retail Stocks Drop After Weak Holiday Sales Forecast\n",
      "Score: 0.1689 | Doc: Cryptocurrency Prices Surge Following Regulatory Clarity in Europe\n"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(qry: str) -> Iterator[Tuple[float, str]]:\n",
    "    preprocessed_query = ' '.join(preprocess(qry))\n",
    "    print(f'Searching for: \"{preprocessed_query}\"')\n",
    "    query_vector = tfidf_vectorizer.transform([preprocessed_query])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = (tfidf_matrix @ query_vector.T).toarray().flatten()\n",
    "    tfidf_ranking = np.argsort(-cosine_sim)\n",
    "    for idx in tfidf_ranking:\n",
    "        if cosine_sim[idx] > 0:\n",
    "            yield cosine_sim[idx], documents[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Tech sector earnings\"\n",
    "query = \"energy supply price drop\"\n",
    "# query = \"automobile sector investments\"\n",
    "\n",
    "print(\"Retrieval results using TF-IDF Ranking:\")\n",
    "for score, result in retrieve_documents(query):\n",
    "    print(f\"Score: {score:.4f} | Doc: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "\n",
    "In the code above, experiment with different queries.\n",
    "Can you find weaknesses with TF-IDF vectorization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues with TF-IDF\n",
    "\n",
    "- **Bag-of-words assumption (no semantics)**  \n",
    "  TF–IDF ignores word order, syntax and meaning. Synonyms and paraphrases (e.g., \"car\" vs \"automobile\") won’t match, and semantically similar documents can be missed.\n",
    "\n",
    "- **Polysemy and ambiguity**  \n",
    "  A single token can have multiple senses; TF–IDF treats them identically and can return irrelevant documents.\n",
    "\n",
    "- **Sparse, high-dimensional vectors**  \n",
    "  Representations are large and sparse, which can be memory- and compute-inefficient at scale, and sensitive to vocabulary mismatch and OOV tokens.\n",
    "\n",
    "- **Length and frequency bias**  \n",
    "  Raw term frequency and document length can bias scores (longer docs more likely to contain query terms). IDF can overemphasize rare noise words if corpus statistics are unstable.\n",
    "\n",
    "- **No contextual or phrase understanding**  \n",
    "  Multi-word expressions and context-dependent meaning are poorly handled; phrase matches are often missed unless explicitly indexed.\n",
    "\n",
    "- **Preprocessing dependency**  \n",
    "  Stemming, lemmatization, stopword lists, tokenization, and case handling can dramatically change results; mismatches in preprocessing between query and documents cause retrieval failures.\n",
    "\n",
    "- **Scaling and sparsity in multilingual/morphologically rich languages**  \n",
    "  Languages with rich morphology, diacritics or compound words pose additional challenges for lexical-only matching.\n",
    "\n",
    "Mitigations include BM25 (better length normalization), query expansion, relevance feedback, LSI/LSA or SVD to capture latent topics, and modern dense (embedding-based) retrieval or hybrid sparse+dense approaches for semantic matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_simple_retreival_TFIDF_BM25",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
