{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval with TF-IDF and BM25\n",
    "\n",
    "*Lexical search* is a type of search where a query is compared against a collection of text documents (as short as a sentence or as long as an entire article) by directly matching the words in the query. For example, given a query like \"What is a cat?\", you would look for documents that contains one or more of the words: \"What\", \"is\", \"a\", \"cat\". You can assign a score to each document based on how important each word is (by assigning a score to each word), and how many words are matched (e.g. by summing up the scores).\n",
    "\n",
    "Traditionally, retrieval was simply a keyword(s) search, aka a boolean search. The retrieval systme just looked for the existance of these words in the document. \n",
    "Soon enough, we learned that it's not enough. While all words were born equal, some are more equal than others.  \n",
    "\n",
    "TF-IDF is an algorithm that learns to reweight the words based on their frequencies: frequent words that appear often in the many documents get less weight, and rare words gets an increased weight.\n",
    "\n",
    "Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31f556f-018c-4997-9116-235c7f7530b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3cc1975-fa8c-47c5-a586-55481d838f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating a Toy-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58eb93cb-93a1-4563-aed8-7dc0f8aba1ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Tech Stocks Rally as Investors Bet on Strong Quarterly Earnings\",\n",
    "    \"Global Markets Dip Amid Concerns Over Slowing Tech Sector Growth\",\n",
    "    \"Cryptocurrency Prices Surge Following Regulatory Clarity in Europe\",\n",
    "    \"Federal Reserve Hints at Future Rate Cuts Boosting Market Confidence\",\n",
    "    \"Oil Prices Fall as Supply Chain Disruptions Ease Worldwide\",\n",
    "    \"Major Bank Reports Record Profits Driven by Consumer Lending\",\n",
    "    \"Retail Stocks Drop After Weak Holiday Sales Forecast\",\n",
    "    \"Automakers Invest Heavily in Electric Vehicles to Stay Competitive\",\n",
    "    \"Investors Pull Back from Risky Assets Amid Inflation Fears\",\n",
    "    \"Fintech Startups Gain Momentum with New Digital Payment Solutions\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e13bc61a-ccf5-4f3d-aa6d-c04e76fefa74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stop Word Removal and Stemming\n",
    "\n",
    "In real-world scenario, we must compromize between the model accuracy and its performance in terms of speed and space. One of the main factors to consuming space is the vocabulary size, the number of different tokens (types) we take into account.\n",
    "\n",
    "There are two main techniques to reduce the number of types:\n",
    "1. Removing stop words\n",
    "2. Stemming and Lemmatization\n",
    "\n",
    "Let's check them out:\n",
    "\n",
    "### Stop Word Removal\n",
    "Stop words are common words in a language (like ```\"the\"```, ```\"is\"```, ```\"and\"```) that appear frequently but carry little semantic meaning. Removing them reduces noise, decrease the number of tokens we must store, and therefore may improve the retrieval efficiency.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Original: ```\"The cat is sitting on the mat.\"```  \n",
    "After Stop Word Removal: ```\"cat sitting mat\"```\n",
    "\n",
    "### Stemming & Lemmatization\n",
    "[*Stemming*](https://www.geeksforgeeks.org/nlp/snowball-stemmer-nlp/) reduces words to their root form by chopping them abruptly, so that different variants of a word are treated as the same token. This helps in reducing the dimensionality, the number of different words we treat, as well as matching similar concepts and words, as they are all mapped to the same word.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Words: ```\"Consult\", \"Consultant\", \"Consulting\", \"Consultantative\", \"Consultants\"```  \n",
    "After Stemming: ```\"consult\"```\n",
    "\n",
    "*Lemmatization* reduces words to their base or dictionary form (aka lemma). Unlike stemming, which simply chops off the end of a word, lemmatization ensure the resulting lemma is a valid word. For example, \"better\" is lemmatized to \"good\" instead of a non-dictionary root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(stop_words=\"english\")\n",
    "stop_words = set(tfidf_vec.get_stop_words())  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1:\n",
    "\n",
    "Complete the preprocessing function in the code chunk below.\n",
    "\n",
    "The input of the function is a string of text.\n",
    "It returns a list of stemmed, non-stop words.\n",
    "\n",
    "1. Split the text into single words. (Hint: use the [```split()```](https://docs.python.org/3/library/stdtypes.html#str.split) function)\n",
    "2. For each word word check if it is a stop word using the ```stop_words``` set.\n",
    "3. Finally stem all the remaining words using [```stemmer.stem()```](https://www.nltk.org/api/nltk.stem.porter.html#nltk.stem.porter.PorterStemmer.stem).\n",
    "4. Return the resulting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str) -> list[str]:\n",
    "    \"\"\"Preprocess the input text by tokenizing, removing stopwords, and stemming.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    tokens = text.lower().split()\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess documents\n",
    "processed_docs = [\" \".join(preprocess(doc)) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "633470cc-81e2-46dc-ad80-b68774844743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## TF-IDF: Term Frequency–Inverse Document Frequency\n",
    "\n",
    "**TF-IDF** is a numerical statistic that reflects how important a word is to a document within a collection (corpus).  \n",
    "It is widely used in information retrieval and text mining to rank documents by relevance.\n",
    "\n",
    "**1. Term Frequency (TF)**\n",
    "\n",
    "Measures how often a term $t$ appears in a document $d$.\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in } d}{\\text{Total number of terms in } d}\n",
    "$$\n",
    "\n",
    "**2. Inverse Document Frequency (IDF)**\n",
    "\n",
    "Measures how informative a term is, by counting the number of documents it appears in — rare terms get a higher score.\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{num of documents containing term t} + 1}\\right)\n",
    "$$\n",
    "\n",
    "(The $+1$ prevents division by zero.)\n",
    "\n",
    "**3. TF-IDF Score**\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "* **High TF-IDF** → The term appears frequently in the document but rarely elsewhere → *important*.\n",
    "* **Low TF-IDF** → The term is common across documents or rare in the current one → *less informative*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF Vectorization\n",
    "\n",
    "Now it is time to apply the TF-IDF vectorization to our preprocessed data. Fortunately, we don't have to code this ourselfs.  \n",
    "The vectorization is implemented for example in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "To perform retrieval, we preprocess and vectorize the query with the same TF-IDF model that we fit on the documents.  \n",
    "Then, a distance metric, such as [*cosine similarity*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html), is used to measure the relevancy of each document, and find the best matching results.\n",
    "\n",
    "$$\\text{Cosine similarity}(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7eaf566-21d4-4be4-a562-d67210d924a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create and fit a TfidfVectorizer over the document corpus\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db13673d-6a94-4b51-b47d-aa99bf411aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_documents(qry: str) -> Iterator[Tuple[float, str]]:\n",
    "    \"\"\"Retrieve documents based on a TF-IDF.\n",
    "\n",
    "    Args:\n",
    "        qry (str): a query to search for\n",
    "\n",
    "    Yields:\n",
    "        Iterator[Tuple[float, str]]: a list of results and their matching score\n",
    "    \"\"\"\n",
    "    preprocessed_query = \" \".join(preprocess(qry))\n",
    "    print(f'Searching for: \"{preprocessed_query}\"')\n",
    "    query_vector = tfidf_vectorizer.transform([preprocessed_query])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = (tfidf_matrix @ query_vector.T).toarray().flatten()\n",
    "    tfidf_ranking = np.argsort(-cosine_sim)\n",
    "    for idx in tfidf_ranking:\n",
    "        if cosine_sim[idx] > 0:\n",
    "            yield cosine_sim[idx], documents[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval results using TF-IDF Ranking:\n",
      "Searching for: \"tech sector earn\"\n",
      "Score: 0.3899 | Doc: Tech Stocks Rally as Investors Bet on Strong Quarterly Earnings\n",
      "Score: 0.3653 | Doc: Global Markets Dip Amid Concerns Over Slowing Tech Sector Growth\n"
     ]
    }
   ],
   "source": [
    "query = \"Tech sector earnings\"\n",
    "# query = \"energy supply price drop\"\n",
    "# query = \"automobile sector investments\"\n",
    "\n",
    "print(\"Retrieval results using TF-IDF Ranking:\")\n",
    "for score, result in retrieve_documents(query):\n",
    "    print(f\"Score: {score:.4f} | Doc: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "\n",
    "In the code above, experiment with different queries.\n",
    "Can you find weaknesses with TF-IDF vectorization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues with TF-IDF\n",
    "\n",
    "- **Bag-of-words assumption (no semantics)**  \n",
    "  TF–IDF ignores word order, syntax and meaning. Synonyms and paraphrases (e.g., \"car\" vs \"automobile\") won’t match, and semantically similar documents can be missed.\n",
    "\n",
    "- **Polysemy and ambiguity**  \n",
    "  A single token can have multiple senses; TF–IDF treats them identically and can return irrelevant documents.\n",
    "\n",
    "- **Sparse, high-dimensional vectors**  \n",
    "  Representations are large and sparse, which can be memory- and compute-inefficient at scale, and sensitive to vocabulary mismatch and OOV tokens.\n",
    "\n",
    "- **Length and frequency bias**  \n",
    "  Raw term frequency and document length can bias scores (longer docs more likely to contain query terms). IDF can overemphasize rare noise words if corpus statistics are unstable.\n",
    "\n",
    "- **No contextual or phrase understanding**  \n",
    "  Multi-word expressions and context-dependent meaning are poorly handled; phrase matches are often missed unless explicitly indexed.\n",
    "\n",
    "- **Preprocessing dependency**  \n",
    "  Stemming, lemmatization, stopword lists, tokenization, and case handling can dramatically change results; mismatches in preprocessing between query and documents cause retrieval failures.\n",
    "\n",
    "- **Scaling and sparsity in multilingual/morphologically rich languages**  \n",
    "  Languages with rich morphology, diacritics or compound words pose additional challenges for lexical-only matching.\n",
    "\n",
    "Mitigations include BM25 (better length normalization), query expansion, relevance feedback, LSI/LSA or SVD to capture latent topics, and modern dense (embedding-based) retrieval or hybrid sparse+dense approaches for semantic matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM-25\n",
    "\n",
    "BM-25 expands on TF-IDF and handles cases where long documents got priority and a higher score, due to their lexical richness. It adds 2 fraction variables over the TF and the IDF, allowing to control their influence on the scores.\n",
    "\n",
    "In this example, we will use [bm25s](https://github.com/xhluca/bm25s). A relatively newer version of BM25, which adds some more capabilities on top of the original algorithm, such as tokenization, and a more efficient score-matrix calculation. You can read more about it in [the paper](https://arxiv.org/abs/2407.03618).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3b7ea124ad403fb655ecbfaa9c030d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6171f0c92ee14d9fa26815675c8c2583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2575dcc8eeb84bbc9f7eb5b31560a33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bm25s\n",
    "\n",
    "# Tokenize the corpus and only keep the ids (faster and saves memory)\n",
    "corpus_tokens = bm25s.tokenize(documents, stopwords=\"en\")\n",
    "\n",
    "# Create the BM25 model and index the corpus\n",
    "retriever = bm25s.BM25()\n",
    "retriever.index(corpus_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents_bm25(qry: str) -> Iterator[Tuple[float, int]]:\n",
    "    \"\"\"Retrieve documents using BM25.\n",
    "\n",
    "    Args:\n",
    "        qry (str): a query to search for\n",
    "\n",
    "    Yields:\n",
    "        Iterator[Tuple[float, str]]: a list of results and their matching score\n",
    "    \"\"\"\n",
    "    preprocessed_query = bm25s.tokenize(qry)\n",
    "    print(f'Searching for: \"{preprocessed_query}\"')\n",
    "    results, scores = retriever.retrieve(preprocessed_query, k=2)\n",
    "\n",
    "    for i in range(results.shape[1]):\n",
    "        score, doc = scores[0, i], results[0, i]\n",
    "        yield score, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval results using BM25 Ranking:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15722e9a5c14107b6388ac2d1b65a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: \"Tokenized(\n",
      "  \"ids\": [\n",
      "    0: [0, 1, 2]\n",
      "  ],\n",
      "  \"vocab\": [\n",
      "    'earnings': 2\n",
      "    'sector': 1\n",
      "    'tech': 0\n",
      "  ],\n",
      ")\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65f03781f884fd6b6d52b3bc4009997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.4050 | Doc: Tech Stocks Rally as Investors Bet on Strong Quarterly Earnings\n",
      "Score: 1.2647 | Doc: Global Markets Dip Amid Concerns Over Slowing Tech Sector Growth\n"
     ]
    }
   ],
   "source": [
    "query = \"Tech sector earnings\"\n",
    "# query = \"energy supply price drop\"\n",
    "# query = \"automobile sector investments\"\n",
    "\n",
    "print(\"Retrieval results using BM25 Ranking:\")\n",
    "for score, result in retrieve_documents_bm25(query):\n",
    "    print(f\"Score: {score:.4f} | Doc: {documents[result]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_simple_retreival_TFIDF_BM25",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
