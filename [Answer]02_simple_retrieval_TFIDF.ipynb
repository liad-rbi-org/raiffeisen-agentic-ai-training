{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31f556f-018c-4997-9116-235c7f7530b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3cc1975-fa8c-47c5-a586-55481d838f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Retrieval with TF-IDF and BM25\n",
    "\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58eb93cb-93a1-4563-aed8-7dc0f8aba1ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Tech Stocks Rally as Investors Bet on Strong Quarterly Earnings\",\n",
    "    \"Global Markets Dip Amid Concerns Over Slowing Tech Sector Growth\",\n",
    "    \"Cryptocurrency Prices Surge Following Regulatory Clarity in Europe\",\n",
    "    \"Federal Reserve Hints at Future Rate Cuts Boosting Market Confidence\",\n",
    "    \"Oil Prices Fall as Supply Chain Disruptions Ease Worldwide\",\n",
    "    \"Major Bank Reports Record Profits Driven by Consumer Lending\",\n",
    "    \"Retail Stocks Drop After Weak Holiday Sales Forecast\",\n",
    "    \"Automakers Invest Heavily in Electric Vehicles to Stay Competitive\",\n",
    "    \"Investors Pull Back from Risky Assets Amid Inflation Fears\",\n",
    "    \"Fintech Startups Gain Momentum with New Digital Payment Solutions\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e13bc61a-ccf5-4f3d-aa6d-c04e76fefa74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stop Word Removal and Stemming\n",
    "\n",
    "### Stop Word Removal\n",
    "Stop words are common words in a language (like ```\"the\"```, ```\"is\"```, ```\"and\"```) that carry little semantic meaning. Removing them reduces noise and often improves the efficiency of text processing.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Original: ```\"The cat is sitting on the mat.\"```\n",
    "After Stop Word Removal: ```\"cat sitting mat\"```\n",
    "\n",
    "### Stemming\n",
    "Stemming reduces words to their root form, so that different variants of a word are treated as the same token. This helps in matching similar concepts.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Words: ```\"running\", \"runs\", \"ran\"```\n",
    "After Stemming: ```\"run\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "715aa317-d14f-4ba9-9e0c-7df46c585935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(TfidfVectorizer(stop_words='english').get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1:\n",
    "\n",
    "Complete the preprocessing function in the code chunk below.\n",
    "\n",
    "The input of the function is a string of text.\n",
    "It returns a list of stemmed, non-stop words.\n",
    "\n",
    "1. Split the text into single words. (Hint: use the [```split()```](https://docs.python.org/3/library/stdtypes.html#str.split) function)\n",
    "2. For each word word check if it is a stop word using the ```stop_words``` set.\n",
    "3. Finally stem all the remaining words using [```stemmer.stem()```](https://www.nltk.org/api/nltk.stem.porter.html#nltk.stem.porter.PorterStemmer.stem).\n",
    "4. Return the resulting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Preprocess the input text by tokenizing, removing stopwords, and stemming.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: A list of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    tokens = text.lower().split()\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess documents\n",
    "processed_docs = [' '.join(preprocess(doc)) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "633470cc-81e2-46dc-ad80-b68774844743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## TF-IDF: Term Frequency–Inverse Document Frequency\n",
    "\n",
    "**TF-IDF** is a numerical statistic that reflects how important a word is to a document within a collection (corpus).\n",
    "It is widely used in information retrieval and text mining to rank documents by relevance.\n",
    "\n",
    "**1. Term Frequency (TF)**\n",
    "\n",
    "Measures how often a term appears in a document.\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in } d}{\\text{Total number of terms in } d}\n",
    "$$\n",
    "\n",
    "**2. Inverse Document Frequency (IDF)**\n",
    "\n",
    "Measures how informative a term is — rare terms get a higher score.\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{N}{1 + n_t}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $N$ = total number of documents\n",
    "* $n_t$ = number of documents containing term *t*\n",
    "\n",
    "(The $+1$ prevents division by zero.)\n",
    "\n",
    "**3. TF-IDF Score**\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "* **High TF-IDF** → The term appears frequently in the document but rarely elsewhere → *important*.\n",
    "* **Low TF-IDF** → The term is common across documents or rare in the current one → *less informative*.\n",
    "\n",
    "## Using TF-IDF Vectorization\n",
    "\n",
    "Now it is time to apply the TF-IDF vectorization to our preprocessed data.\n",
    "Fortunately we don't have to code this ourselfs.\n",
    "The vectorization is implemented for example in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "We vectorize the query the same way as the documents and calculate the TF-IDF vectorization.\n",
    "Then the cosine similarity can be used to find the best matching results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7eaf566-21d4-4be4-a562-d67210d924a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db13673d-6a94-4b51-b47d-aa99bf411aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Ranking:\n",
      "Score: 0.2673 | Doc: Automakers Invest Heavily in Electric Vehicles to Stay Competitive\n",
      "Score: 0.2474 | Doc: Global Markets Dip Amid Concerns Over Slowing Tech Sector Growth\n",
      "Score: 0.0000 | Doc: Tech Stocks Rally as Investors Bet on Strong Quarterly Earnings\n",
      "Score: 0.0000 | Doc: Cryptocurrency Prices Surge Following Regulatory Clarity in Europe\n",
      "Score: 0.0000 | Doc: Federal Reserve Hints at Future Rate Cuts Boosting Market Confidence\n",
      "Score: 0.0000 | Doc: Oil Prices Fall as Supply Chain Disruptions Ease Worldwide\n",
      "Score: 0.0000 | Doc: Major Bank Reports Record Profits Driven by Consumer Lending\n",
      "Score: 0.0000 | Doc: Retail Stocks Drop After Weak Holiday Sales Forecast\n",
      "Score: 0.0000 | Doc: Investors Pull Back from Risky Assets Amid Inflation Fears\n",
      "Score: 0.0000 | Doc: Fintech Startups Gain Momentum with New Digital Payment Solutions\n"
     ]
    }
   ],
   "source": [
    "query = \"tech sector earnings\"\n",
    "# query = \"energy supply price drop\"\n",
    "# query = \"automobile sector investments\"\n",
    "preprocessed_query = ' '.join(preprocess(query))\n",
    "query_vector = tfidf_vectorizer.transform([preprocessed_query])\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = (tfidf_matrix @ query_vector.T).toarray().flatten()\n",
    "tfidf_ranking = np.argsort(-cosine_sim)\n",
    "\n",
    "print(\"TF-IDF Ranking:\")\n",
    "for idx in tfidf_ranking:\n",
    "    print(f\"Score: {cosine_sim[idx]:.4f} | Doc: {documents[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "\n",
    "In the code above, experiment with different queries.\n",
    "Can you find weaknesses with TF-IDF vectorization?\n",
    "\n",
    "### Issues with TF-IDF\n",
    "\n",
    "- **Bag-of-words assumption (no semantics)**  \n",
    "  TF–IDF ignores word order, syntax and meaning. Synonyms and paraphrases (e.g., \"car\" vs \"automobile\") won’t match, and semantically similar documents can be missed.\n",
    "\n",
    "- **Polysemy and ambiguity**  \n",
    "  A single token can have multiple senses; TF–IDF treats them identically and can return irrelevant documents.\n",
    "\n",
    "- **Sparse, high-dimensional vectors**  \n",
    "  Representations are large and sparse, which can be memory- and compute-inefficient at scale, and sensitive to vocabulary mismatch and OOV tokens.\n",
    "\n",
    "- **Length and frequency bias**  \n",
    "  Raw term frequency and document length can bias scores (longer docs more likely to contain query terms). IDF can overemphasize rare noise words if corpus statistics are unstable.\n",
    "\n",
    "- **No contextual or phrase understanding**  \n",
    "  Multi-word expressions and context-dependent meaning are poorly handled; phrase matches are often missed unless explicitly indexed.\n",
    "\n",
    "- **Preprocessing dependency**  \n",
    "  Stemming, lemmatization, stopword lists, tokenization, and case handling can dramatically change results; mismatches in preprocessing between query and documents cause retrieval failures.\n",
    "\n",
    "- **Scaling and sparsity in multilingual/morphologically rich languages**  \n",
    "  Languages with rich morphology, diacritics or compound words pose additional challenges for lexical-only matching.\n",
    "\n",
    "Mitigations include BM25 (better length normalization), query expansion, relevance feedback, LSI/LSA or SVD to capture latent topics, and modern dense (embedding-based) retrieval or hybrid sparse+dense approaches for semantic matching."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_simple_retreival_TFIDF_BM25",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
